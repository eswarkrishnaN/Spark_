{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "269de17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1c7810",
   "metadata": {},
   "source": [
    "    you create a SparkSession with a specific application name, allowing you to leverage the functionality of Spark for processing and analyzing data in a distributed manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "400920a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/02 09:30:42 WARN Utils: Your hostname, parrot-aspirees114141 resolves to a loopback address: 127.0.1.1; using 172.20.10.3 instead (on interface wlp5s0)\n",
      "23/06/02 09:30:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/02 09:30:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master('local[*]')\n",
    "         .appName(\"creating_a_new_one\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7eac105",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3dfba240",
   "metadata": {},
   "source": [
    "       Overall, the sparkContext object serves as the bridge between the Spark application and the Spark cluster. It enables developers to leverage the distributed computing capabilities of Spark and perform various data processing operations efficiently "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145847e6",
   "metadata": {},
   "source": [
    "    for example Cluster Connection, Job Execution , Data Distribution , Resilience (fault tolerance capabilities),Resource Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b10bf99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.20.10.3:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>creating_a_new_one</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=creating_a_new_one>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0f12dc",
   "metadata": {},
   "source": [
    "    most we set this what was the loglevel it was default set to warn it ok you but you need to see more these are the options  \"OFF\" , \"FATAL\" , \"ERROR\" ,\"WARN\" ,\"INFO\" ,\"DEBUG\" ,\"TRACE\" ,\"ALL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00e5669a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad2240b",
   "metadata": {},
   "source": [
    "    Reading data into a data frame is done through the DataFrameReader object, which we can access through spark.read   \n",
    "  'csv',\n",
    " 'format',\n",
    " 'jdbc',\n",
    " 'json',\n",
    " 'load',\n",
    " 'option',\n",
    " 'options',\n",
    " 'orc',\n",
    " 'parquet',\n",
    " 'schema',\n",
    " 'table',\n",
    " 'text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56a79133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_df',\n",
       " '_jreader',\n",
       " '_set_opts',\n",
       " '_spark',\n",
       " 'csv',\n",
       " 'format',\n",
       " 'jdbc',\n",
       " 'json',\n",
       " 'load',\n",
       " 'option',\n",
       " 'options',\n",
       " 'orc',\n",
       " 'parquet',\n",
       " 'schema',\n",
       " 'table',\n",
       " 'text']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read\n",
    "\n",
    "dir(spark.read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cb07b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+----------+----------+----------+----------+----------+-------+----------+\n",
      "| _c0|       _c1|       _c2|       _c3|       _c4|       _c5|       _c6|       _c7|    _c8|       _c9|\n",
      "+----+----------+----------+----------+----------+----------+----------+----------+-------+----------+\n",
      "|Year|Industr...|Industr...|Industr...|     Units|Variabl...|Variabl...|Variabl...|  Value|Industr...|\n",
      "|2021|   Level 1|     99999|All ind...|Dollars...|       H01|Total i...|Financi...|757,504|ANZSIC0...|\n",
      "+----+----------+----------+----------+----------+----------+----------+----------+-------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "book = spark.read.option(\"header\" , \"true\").format(\"csv\").load(\"some.csv\")\n",
    "\n",
    "# you can also insert data using\n",
    "book = spark.read.csv(\"some.csv\")\n",
    "\n",
    "#spark until to tell to start the process it will not run the porcess\n",
    "\n",
    "book.show(2 , truncate=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a58468e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#you can view the columns are which datatyeps using printSchema\n",
    "\n",
    "book.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f54c3b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('_c0', 'string'), ('_c1', 'string'), ('_c2', 'string'), ('_c3', 'string'), ('_c4', 'string'), ('_c5', 'string'), ('_c6', 'string'), ('_c7', 'string'), ('_c8', 'string'), ('_c9', 'string')]\n"
     ]
    }
   ],
   "source": [
    "print(book.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c900457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The entry point to programming Spark with the Dataset and DataFrame API.\n",
      "\n",
      "    A SparkSession can be used create :class:`DataFrame`, register :class:`DataFrame` as\n",
      "    tables, execute SQL over tables, cache tables, and read parquet files.\n",
      "    To create a :class:`SparkSession`, use the following builder pattern:\n",
      "\n",
      "    .. autoattribute:: builder\n",
      "       :annotation:\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> spark = SparkSession.builder \\\n",
      "    ...     .master(\"local\") \\\n",
      "    ...     .appName(\"Word Count\") \\\n",
      "    ...     .config(\"spark.some.config.option\", \"some-value\") \\\n",
      "    ...     .getOrCreate()\n",
      "\n",
      "    >>> from datetime import datetime\n",
      "    >>> from pyspark.sql import Row\n",
      "    >>> spark = SparkSession(sc)\n",
      "    >>> allTypes = sc.parallelize([Row(i=1, s=\"string\", d=1.0, l=1,\n",
      "    ...     b=True, list=[1, 2, 3], dict={\"s\": 0}, row=Row(a=1),\n",
      "    ...     time=datetime(2014, 8, 1, 14, 1, 5))])\n",
      "    >>> df = allTypes.toDF()\n",
      "    >>> df.createOrReplaceTempView(\"allTypes\")\n",
      "    >>> spark.sql('select i+1, d+1, not b, list[1], dict[\"s\"], time, row.a '\n",
      "    ...            'from allTypes where b and i > 0').collect()\n",
      "    [Row((i + 1)=2, (d + 1)=2.0, (NOT b)=False, list[1]=2,         dict[s]=0, time=datetime.datetime(2014, 8, 1, 14, 1, 5), a=1)]\n",
      "    >>> df.rdd.map(lambda x: (x.i, x.s, x.d, x.l, x.b, x.time, x.row.a, x.list)).collect()\n",
      "    [(1, 'string', 1.0, 1, True, datetime.datetime(2014, 8, 1, 14, 1, 5), 1, [1, 2, 3])]\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(spark.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46719609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| _c0|\n",
      "+----+\n",
      "|Year|\n",
      "|2021|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "#you can select using column using select are col \n",
    "\n",
    "book.select(book._c0)\n",
    "\n",
    "\n",
    "book.select(book[\"_c0\"])\n",
    "\n",
    "#this approce is more flexbile and usefull\n",
    "book.select(col(\"_c0\")).show(2 , truncate=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3f4e6974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- split(_c7,  , -1): array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      "\n",
      "+------------------------+\n",
      "|split(_c7,  , -1)       |\n",
      "+------------------------+\n",
      "|[Variable_category]     |\n",
      "|[Financial, performance]|\n",
      "|[Financial, performance]|\n",
      "|[Financial, performance]|\n",
      "|[Financial, performance]|\n",
      "+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "clean = book.select(split(col(\"_c7\"), \" \"))\n",
    "\n",
    "#it also changes an array convert to string\n",
    "clean.printSchema()\n",
    "\n",
    "#it will by space using comma \n",
    "clean.show(5 , truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6a75f166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+--------------------+------------------+-------------+-------------+--------------------+-------+--------------------+\n",
      "| _c0|                 _c1|                 _c2|                 _c3|               _c4|          _c5|          _c6|             seventh|    _c8|                 _c9|\n",
      "+----+--------------------+--------------------+--------------------+------------------+-------------+-------------+--------------------+-------+--------------------+\n",
      "|Year|Industry_aggregat...|Industry_code_NZSIOC|Industry_name_NZSIOC|             Units|Variable_code|Variable_name|   Variable_category|  Value|Industry_code_ANZ...|\n",
      "|2021|             Level 1|               99999|      All industries|Dollars (millions)|          H01| Total income|Financial perform...|757,504|ANZSIC06 division...|\n",
      "+----+--------------------+--------------------+--------------------+------------------+-------------+-------------+--------------------+-------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#WithColumnRenamed are alias used same to do renameing\n",
    "\n",
    "\n",
    "rename = book.withColumnRenamed(\"_c7\",\"seventh\")\n",
    "\n",
    "#same as the you can also set with alias\n",
    "\n",
    "\n",
    "\n",
    "book.select(col('_c6')).alias(\"sith\")\n",
    "\n",
    "rename.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4233741a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [69]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#explode fucntions to convert to list\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# these got error it was in list formate\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m words \u001b[38;5;241m=\u001b[39m book\u001b[38;5;241m.\u001b[39mselect(\u001b[43mexplode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_c6\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlist\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "#explode fucntions to convert to list\n",
    "\n",
    "\n",
    "# these got error it was in list formate\n",
    "words = book.select(explode(col(\"_c6\")).alias(\"list\").show(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "de0ea8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|lowercase|\n",
      "+---------+\n",
      "|  ariable|\n",
      "|     otal|\n",
      "|     ales|\n",
      "|  nterest|\n",
      "|       on|\n",
      "+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#regexp_extract you can do much more of things using this function like convert text to lover\n",
    "#remove punction are symboles etc\n",
    "\n",
    "word_clean = book.select(regexp_extract(col(\"_c6\"), \"[a-z]+\" , 0).alias('lowercase'))\n",
    "\n",
    "word_clean.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c334f645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2502\n"
     ]
    }
   ],
   "source": [
    "#filtering fucntion use to filter the row\n",
    "\n",
    "selected = word_clean.filter(col('lowercase') == \"on\").count()\n",
    "\n",
    "print(selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cda968a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|length(_c6)|\n",
      "+-----------+\n",
      "|         13|\n",
      "|         12|\n",
      "+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#it will show the length of the coloum\n",
    "\n",
    "selected = book.select(length(col('_c6'))).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0ca2927c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|greatest(length(_c1), length(_c6))|\n",
      "+----------------------------------+\n",
      "|                                27|\n",
      "|                                12|\n",
      "|                                47|\n",
      "|                                33|\n",
      "|                                20|\n",
      "|                                17|\n",
      "|                                22|\n",
      "|                                14|\n",
      "|                                12|\n",
      "|                                23|\n",
      "|                                24|\n",
      "|                                53|\n",
      "|                                38|\n",
      "|                                22|\n",
      "|                                14|\n",
      "|                                14|\n",
      "|                                25|\n",
      "|                                12|\n",
      "|                                14|\n",
      "|                                21|\n",
      "+----------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# you can also set the greatest values using this key\n",
    "\n",
    "\n",
    "greter_than = book.select(greatest(length(col('_c1')) ,length(col('_c6')) ).alias(\"values\"))\n",
    "\n",
    "greter_than.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
