{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a07946a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbb6736",
   "metadata": {},
   "source": [
    "    you create a SparkSession with a specific application name, allowing you to leverage the functionality of Spark for processing and analyzing data in a distributed manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ac928604",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master('local[*]')\n",
    "         .appName(\"creating_a_new_one\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64a78dd",
   "metadata": {},
   "source": [
    "       Overall, the sparkContext object serves as the bridge between the Spark application and the Spark cluster. It enables developers to leverage the distributed computing capabilities of Spark and perform various data processing operations efficiently "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc1fdac",
   "metadata": {},
   "source": [
    "    for example Cluster Connection, Job Execution , Data Distribution , Resilience (fault tolerance capabilities),Resource Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d6f753af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.20.10.3:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>creating_a_new_one</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=creating_a_new_one>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11be0a2c",
   "metadata": {},
   "source": [
    "    most we set this what was the loglevel it was default set to warn it ok you but you need to see more these are the options  \"OFF\" , \"FATAL\" , \"ERROR\" ,\"WARN\" ,\"INFO\" ,\"DEBUG\" ,\"TRACE\" ,\"ALL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dcd3012",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649a162b",
   "metadata": {},
   "source": [
    "    Reading data into a data frame is done through the DataFrameReader object, which we can access through spark.read   \n",
    "  'csv',\n",
    " 'format',\n",
    " 'jdbc',\n",
    " 'json',\n",
    " 'load',\n",
    " 'option',\n",
    " 'options',\n",
    " 'orc',\n",
    " 'parquet',\n",
    " 'schema',\n",
    " 'table',\n",
    " 'text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91577cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_df',\n",
       " '_jreader',\n",
       " '_set_opts',\n",
       " '_spark',\n",
       " 'csv',\n",
       " 'format',\n",
       " 'jdbc',\n",
       " 'json',\n",
       " 'load',\n",
       " 'option',\n",
       " 'options',\n",
       " 'orc',\n",
       " 'parquet',\n",
       " 'schema',\n",
       " 'table',\n",
       " 'text']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read\n",
    "\n",
    "dir(spark.read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "24f98098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------------+--------------------+--------------------+------------------+-------------+-------------+---------------------+-------+----------------------------------------------------------------------------------------------------------------+\n",
      "|_c0 |_c1                        |_c2                 |_c3                 |_c4               |_c5          |_c6          |_c7                  |_c8    |_c9                                                                                                             |\n",
      "+----+---------------------------+--------------------+--------------------+------------------+-------------+-------------+---------------------+-------+----------------------------------------------------------------------------------------------------------------+\n",
      "|Year|Industry_aggregation_NZSIOC|Industry_code_NZSIOC|Industry_name_NZSIOC|Units             |Variable_code|Variable_name|Variable_category    |Value  |Industry_code_ANZSIC06                                                                                          |\n",
      "|2021|Level 1                    |99999               |All industries      |Dollars (millions)|H01          |Total income |Financial performance|757,504|ANZSIC06 divisions A-S (excluding classes K6330, L6711, O7552, O760, O771, O772, S9540, S9601, S9602, and S9603)|\n",
      "+----+---------------------------+--------------------+--------------------+------------------+-------------+-------------+---------------------+-------+----------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "book = spark.read.format(\"csv\").load(\"some.csv\")\n",
    "\n",
    "# you can also insert data using\n",
    "book = spark.read.csv(\"some.csv\")\n",
    "\n",
    "#spark until to tell to start the process it will not run the porcess\n",
    "\n",
    "book.show(2 , truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6c6c2c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#you can view the columns are which datatyeps using printSchema\n",
    "\n",
    "book.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "266e0d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('_c0', 'string'), ('_c1', 'string'), ('_c2', 'string'), ('_c3', 'string'), ('_c4', 'string'), ('_c5', 'string'), ('_c6', 'string'), ('_c7', 'string'), ('_c8', 'string'), ('_c9', 'string')]\n"
     ]
    }
   ],
   "source": [
    "print(book.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c54f10ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The entry point to programming Spark with the Dataset and DataFrame API.\n",
      "\n",
      "    A SparkSession can be used create :class:`DataFrame`, register :class:`DataFrame` as\n",
      "    tables, execute SQL over tables, cache tables, and read parquet files.\n",
      "    To create a :class:`SparkSession`, use the following builder pattern:\n",
      "\n",
      "    .. autoattribute:: builder\n",
      "       :annotation:\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> spark = SparkSession.builder \\\n",
      "    ...     .master(\"local\") \\\n",
      "    ...     .appName(\"Word Count\") \\\n",
      "    ...     .config(\"spark.some.config.option\", \"some-value\") \\\n",
      "    ...     .getOrCreate()\n",
      "\n",
      "    >>> from datetime import datetime\n",
      "    >>> from pyspark.sql import Row\n",
      "    >>> spark = SparkSession(sc)\n",
      "    >>> allTypes = sc.parallelize([Row(i=1, s=\"string\", d=1.0, l=1,\n",
      "    ...     b=True, list=[1, 2, 3], dict={\"s\": 0}, row=Row(a=1),\n",
      "    ...     time=datetime(2014, 8, 1, 14, 1, 5))])\n",
      "    >>> df = allTypes.toDF()\n",
      "    >>> df.createOrReplaceTempView(\"allTypes\")\n",
      "    >>> spark.sql('select i+1, d+1, not b, list[1], dict[\"s\"], time, row.a '\n",
      "    ...            'from allTypes where b and i > 0').collect()\n",
      "    [Row((i + 1)=2, (d + 1)=2.0, (NOT b)=False, list[1]=2,         dict[s]=0, time=datetime.datetime(2014, 8, 1, 14, 1, 5), a=1)]\n",
      "    >>> df.rdd.map(lambda x: (x.i, x.s, x.d, x.l, x.b, x.time, x.row.a, x.list)).collect()\n",
      "    [(1, 'string', 1.0, 1, True, datetime.datetime(2014, 8, 1, 14, 1, 5), 1, [1, 2, 3])]\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(spark.__doc__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
