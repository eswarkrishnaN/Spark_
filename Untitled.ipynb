{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b46aeb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f3af4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ipyparallel as ipp\n",
    "\n",
    "cluster = ipp.Cluster(n=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1264ae15",
   "metadata": {},
   "source": [
    "    you create a SparkSession with a specific application name, allowing you to leverage the functionality of Spark for processing and analyzing data in a distributed manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53073a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/02 15:46:03 WARN Utils: Your hostname, parrot-aspirees114141 resolves to a loopback address: 127.0.1.1, but we couldn't find any external IP address!\n",
      "23/06/02 15:46:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/02 15:46:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/06/02 15:46:04 WARN MacAddressUtil: Failed to find a usable hardware address from the network interfaces; using random bytes: 89:50:06:0f:0b:3e:1d:0b\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master('local[*]')\n",
    "         .appName(\"creating_a_new_one\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ab80c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6463299d",
   "metadata": {},
   "source": [
    "       Overall, the sparkContext object serves as the bridge between the Spark application and the Spark cluster. It enables developers to leverage the distributed computing capabilities of Spark and perform various data processing operations efficiently "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b099e12",
   "metadata": {},
   "source": [
    "    for example Cluster Connection, Job Execution , Data Distribution , Resilience (fault tolerance capabilities),Resource Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1174079b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://parrot-aspirees114141:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>creating_a_new_one</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=creating_a_new_one>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c0b77d",
   "metadata": {},
   "source": [
    "    most we set this what was the loglevel it was default set to warn it ok you but you need to see more these are the options  \"OFF\" , \"FATAL\" , \"ERROR\" ,\"WARN\" ,\"INFO\" ,\"DEBUG\" ,\"TRACE\" ,\"ALL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9c208f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b322c270",
   "metadata": {},
   "source": [
    "    Reading data into a data frame is done through the DataFrameReader object, which we can access through spark.read   \n",
    "  'csv',\n",
    " 'format',\n",
    " 'jdbc',\n",
    " 'json',\n",
    " 'load',\n",
    " 'option',\n",
    " 'options',\n",
    " 'orc',\n",
    " 'parquet',\n",
    " 'schema',\n",
    " 'table',\n",
    " 'text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92ec0ca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_df',\n",
       " '_jreader',\n",
       " '_set_opts',\n",
       " '_spark',\n",
       " 'csv',\n",
       " 'format',\n",
       " 'jdbc',\n",
       " 'json',\n",
       " 'load',\n",
       " 'option',\n",
       " 'options',\n",
       " 'orc',\n",
       " 'parquet',\n",
       " 'schema',\n",
       " 'table',\n",
       " 'text']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read\n",
    "\n",
    "dir(spark.read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "778b4b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+----------+----------+----------+----------+----------+-------+----------+\n",
      "| _c0|       _c1|       _c2|       _c3|       _c4|       _c5|       _c6|       _c7|    _c8|       _c9|\n",
      "+----+----------+----------+----------+----------+----------+----------+----------+-------+----------+\n",
      "|Year|Industr...|Industr...|Industr...|     Units|Variabl...|Variabl...|Variabl...|  Value|Industr...|\n",
      "|2021|   Level 1|     99999|All ind...|Dollars...|       H01|Total i...|Financi...|757,504|ANZSIC0...|\n",
      "+----+----------+----------+----------+----------+----------+----------+----------+-------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "book = spark.read.option(\"header\" , \"true\").format(\"csv\").load(\"some.csv\")\n",
    "\n",
    "# you can also insert data using\n",
    "book = spark.read.csv(\"some.csv\")\n",
    "\n",
    "#spark until to tell to start the process it will not run the porcess\n",
    "\n",
    "book.show(2 , truncate=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8576d9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#you can view the columns are which datatyeps using printSchema\n",
    "\n",
    "book.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d27d27bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('_c0', 'string'), ('_c1', 'string'), ('_c2', 'string'), ('_c3', 'string'), ('_c4', 'string'), ('_c5', 'string'), ('_c6', 'string'), ('_c7', 'string'), ('_c8', 'string'), ('_c9', 'string')]\n"
     ]
    }
   ],
   "source": [
    "print(book.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a721677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The entry point to programming Spark with the Dataset and DataFrame API.\n",
      "\n",
      "    A SparkSession can be used create :class:`DataFrame`, register :class:`DataFrame` as\n",
      "    tables, execute SQL over tables, cache tables, and read parquet files.\n",
      "    To create a :class:`SparkSession`, use the following builder pattern:\n",
      "\n",
      "    .. autoattribute:: builder\n",
      "       :annotation:\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> spark = SparkSession.builder \\\n",
      "    ...     .master(\"local\") \\\n",
      "    ...     .appName(\"Word Count\") \\\n",
      "    ...     .config(\"spark.some.config.option\", \"some-value\") \\\n",
      "    ...     .getOrCreate()\n",
      "\n",
      "    >>> from datetime import datetime\n",
      "    >>> from pyspark.sql import Row\n",
      "    >>> spark = SparkSession(sc)\n",
      "    >>> allTypes = sc.parallelize([Row(i=1, s=\"string\", d=1.0, l=1,\n",
      "    ...     b=True, list=[1, 2, 3], dict={\"s\": 0}, row=Row(a=1),\n",
      "    ...     time=datetime(2014, 8, 1, 14, 1, 5))])\n",
      "    >>> df = allTypes.toDF()\n",
      "    >>> df.createOrReplaceTempView(\"allTypes\")\n",
      "    >>> spark.sql('select i+1, d+1, not b, list[1], dict[\"s\"], time, row.a '\n",
      "    ...            'from allTypes where b and i > 0').collect()\n",
      "    [Row((i + 1)=2, (d + 1)=2.0, (NOT b)=False, list[1]=2,         dict[s]=0, time=datetime.datetime(2014, 8, 1, 14, 1, 5), a=1)]\n",
      "    >>> df.rdd.map(lambda x: (x.i, x.s, x.d, x.l, x.b, x.time, x.row.a, x.list)).collect()\n",
      "    [(1, 'string', 1.0, 1, True, datetime.datetime(2014, 8, 1, 14, 1, 5), 1, [1, 2, 3])]\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(spark.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62b9efb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| _c0|\n",
      "+----+\n",
      "|Year|\n",
      "|2021|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "#you can select using column using select are col \n",
    "\n",
    "book.select(book._c0)\n",
    "\n",
    "\n",
    "book.select(book[\"_c0\"])\n",
    "\n",
    "#this approce is more flexbile and usefull\n",
    "book.select(col(\"_c0\")).show(2 , truncate=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d060cd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- split(_c7,  , -1): array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      "\n",
      "+------------------------+\n",
      "|split(_c7,  , -1)       |\n",
      "+------------------------+\n",
      "|[Variable_category]     |\n",
      "|[Financial, performance]|\n",
      "|[Financial, performance]|\n",
      "|[Financial, performance]|\n",
      "|[Financial, performance]|\n",
      "+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "clean = book.select(split(col(\"_c7\"), \" \"))\n",
    "\n",
    "#it also changes an array convert to string\n",
    "clean.printSchema()\n",
    "\n",
    "#it will by space using comma \n",
    "clean.show(5 , truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5682c3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+--------------------+------------------+-------------+-------------+--------------------+-------+--------------------+\n",
      "| _c0|                 _c1|                 _c2|                 _c3|               _c4|          _c5|          _c6|             seventh|    _c8|                 _c9|\n",
      "+----+--------------------+--------------------+--------------------+------------------+-------------+-------------+--------------------+-------+--------------------+\n",
      "|Year|Industry_aggregat...|Industry_code_NZSIOC|Industry_name_NZSIOC|             Units|Variable_code|Variable_name|   Variable_category|  Value|Industry_code_ANZ...|\n",
      "|2021|             Level 1|               99999|      All industries|Dollars (millions)|          H01| Total income|Financial perform...|757,504|ANZSIC06 division...|\n",
      "+----+--------------------+--------------------+--------------------+------------------+-------------+-------------+--------------------+-------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#WithColumnRenamed are alias used same to do renameing\n",
    "\n",
    "\n",
    "rename = book.withColumnRenamed(\"_c7\",\"seventh\")\n",
    "\n",
    "#same as the you can also set with alias\n",
    "\n",
    "\n",
    "\n",
    "book.select(col('_c6')).alias(\"sith\")\n",
    "\n",
    "rename.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "044fa6d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#explode fucntions to convert to list\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# these got error it was in list formate\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m words \u001b[38;5;241m=\u001b[39m book\u001b[38;5;241m.\u001b[39mselect(\u001b[43mexplode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_c6\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlist\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "#explode fucntions to convert to list\n",
    "\n",
    "\n",
    "# these got error it was in list formate\n",
    "words = book.select(explode(col(\"_c6\")).alias(\"list\").show(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e03ce509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|lowercase|\n",
      "+---------+\n",
      "|  ariable|\n",
      "|     otal|\n",
      "|     ales|\n",
      "|  nterest|\n",
      "|       on|\n",
      "+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#regexp_extract you can do much more of things using this function like convert text to lover\n",
    "#remove punction are symboles etc\n",
    "\n",
    "word_clean = book.select(regexp_extract(col(\"_c6\"), \"[a-z]+\" , 0).alias('lowercase'))\n",
    "\n",
    "word_clean.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a706011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2502\n"
     ]
    }
   ],
   "source": [
    "#filtering fucntion use to filter the row\n",
    "\n",
    "selected = word_clean.filter(col('lowercase') == \"on\").count()\n",
    "\n",
    "print(selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "905d9d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|length(_c6)|\n",
      "+-----------+\n",
      "|         13|\n",
      "|         12|\n",
      "+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#it will show the length of the coloum\n",
    "\n",
    "selected = book.select(length(col('_c6'))).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac6a0fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|values|\n",
      "+------+\n",
      "|    27|\n",
      "|    12|\n",
      "|    47|\n",
      "|    33|\n",
      "|    20|\n",
      "|    17|\n",
      "|    22|\n",
      "|    14|\n",
      "|    12|\n",
      "|    23|\n",
      "|    24|\n",
      "|    53|\n",
      "|    38|\n",
      "|    22|\n",
      "|    14|\n",
      "|    14|\n",
      "|    25|\n",
      "|    12|\n",
      "|    14|\n",
      "|    21|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+\n",
      "|              values|\n",
      "+--------------------+\n",
      "|industry_aggregat...|\n",
      "|             level 1|\n",
      "|             level 1|\n",
      "|             level 1|\n",
      "|             level 1|\n",
      "|             level 1|\n",
      "|             level 1|\n",
      "|             level 1|\n",
      "|             level 1|\n",
      "|             level 1|\n",
      "|             level 1|\n",
      "|             level 1|\n",
      "|             level 1|\n",
      "|             level 1|\n",
      "|             level 1|\n",
      "|             level 1|\n",
      "|             level 1|\n",
      "|             level 1|\n",
      "|             level 1|\n",
      "|             level 1|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# you can also set the greatest values using this key\n",
    "\n",
    "#you can also use lower case convert the text\n",
    "\n",
    "greter_than = book.select(greatest(length(col('_c1')) ,length(col('_c6')) ).alias(\"values\"))\n",
    "\n",
    "greter_than.show()\n",
    "\n",
    "lower_value = book.select(lower(col('_c1')) .alias(\"values\"))\n",
    "\n",
    "lower_value.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64a9c6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|  _c2|count|\n",
      "+-----+-----+\n",
      "|CC211|  324|\n",
      "|CC611|  324|\n",
      "|II131|  297|\n",
      "| GH21|  324|\n",
      "| MN21|  297|\n",
      "|AA141|  270|\n",
      "|LL112|  306|\n",
      "|   MN|  297|\n",
      "| II12|  297|\n",
      "| CC81|  324|\n",
      "|   AA|  279|\n",
      "|CC311|  324|\n",
      "|QQ113|  270|\n",
      "|MN213|  297|\n",
      "| AA12|  270|\n",
      "| GH12|  324|\n",
      "|GH135|  324|\n",
      "|   EE|  297|\n",
      "| KK13|  225|\n",
      "|OO213|  279|\n",
      "+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#groupby is function same in sql it group the same simllaer values and print the output\n",
    "\n",
    "group = book.groupby(col('_c2')).count()\n",
    "group.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6544a600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+--------------------+------------------+-------------+-------------+--------------------+-------+--------------------+\n",
      "| _c0|                 _c1|                 _c2|                 _c3|               _c4|          _c5|          _c6|                 _c7|    _c8|                 _c9|\n",
      "+----+--------------------+--------------------+--------------------+------------------+-------------+-------------+--------------------+-------+--------------------+\n",
      "|Year|Industry_aggregat...|Industry_code_NZSIOC|Industry_name_NZSIOC|             Units|Variable_code|Variable_name|   Variable_category|  Value|Industry_code_ANZ...|\n",
      "|2021|             Level 1|               99999|      All industries|Dollars (millions)|          H01| Total income|Financial perform...|757,504|ANZSIC06 division...|\n",
      "+----+--------------------+--------------------+--------------------+------------------+-------------+-------------+--------------------+-------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Dataset in ascending or descending order based on one or more columns. I\n",
    "\n",
    "\n",
    "orderby = book.orderBy(\"_c0\" , ascending=False).show(2)\n",
    "\n",
    "#ouput it will give the values  sort by you selected column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2872388f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|length(_c7)|count|\n",
      "+-----------+-----+\n",
      "|         16| 8919|\n",
      "|         17|    1|\n",
      "|         21|20853|\n",
      "|         18|11943|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "example = (book.orderBy(\"_c1\",ascending=True)\n",
    "           .groupby(length(col(\"_c7\")))\n",
    "                           .count()\n",
    "                           .show(5))\n",
    "\n",
    "\n",
    "book.write.mode(\"overwrite\").parquet(\"spark\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1859cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the coalesce() you can the practications\n",
    "\n",
    "col = book.coalesce(1)\n",
    "\n",
    "col.write.mode(\"overwrite\").parquet(\"spark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a438d0e7",
   "metadata": {},
   "source": [
    "# Reading text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0ab8aa6",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3623321995.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [24]\u001b[0;36m\u001b[0m\n\u001b[0;31m    timestampFormat=\"yyyy-MM-dd\"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#some example of reading data option inferSchema is read datastructer\n",
    "#sep is spreater file \n",
    "#header is heading the file \n",
    "#timestampFormat is used to inform to parser of the formate\n",
    "\n",
    "sample = spark.read.csv([...],\n",
    "                        sep=[...],\n",
    "                        header=[...],\n",
    "                        quote=[...],\n",
    "                        inferSchema=[...]\n",
    "                        timestampFormat=\"yyyy-MM-dd\"\n",
    ")\n",
    "\n",
    "#it will show all the data\n",
    "sample.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "839d316f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#it will selected the column\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mlogs\u001b[49m\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBroadcastLogID\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogServiceID\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogDate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Using the string to column conversion\u001b[39;00m\n\u001b[1;32m      5\u001b[0m logs\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBroadCastLogID\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogServiceID\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogDate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logs' is not defined"
     ]
    }
   ],
   "source": [
    "#it will selected the column\n",
    "logs.select(\"BroadcastLogID\", \"LogServiceID\", \"LogDate\").show(5, False)\n",
    "\n",
    "# Using the string to column conversion\n",
    "logs.select(\"BroadCastLogID\", \"LogServiceID\", \"LogDate\")\n",
    "logs.select(*[\"BroadCastLogID\", \"LogServiceID\", \"LogDate\"])\n",
    " \n",
    "# Passing the column object explicitly\n",
    "logs.select(\n",
    "    F.col(\"BroadCastLogID\"), F.col(\"LogServiceID\"), F.col(\"LogDate\")\n",
    ")\n",
    "logs.select(\n",
    "    *[F.col(\"BroadCastLogID\"), F.col(\"LogServiceID\"), F.col(\"LogDate\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "207d6361",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#we can also drop the columns using drop function\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[43mlogs\u001b[49m\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBroadcastLogID\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSequenceNo\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBroadcastLogID\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m logs\u001b[38;5;241m.\u001b[39mcolumns)  \u001b[38;5;66;03m# => False\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSequenceNo\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m logs\u001b[38;5;241m.\u001b[39mcolumns)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logs' is not defined"
     ]
    }
   ],
   "source": [
    "#we can also drop the columns using drop function\n",
    "\n",
    "logs = logs.drop(\"BroadcastLogID\",\"SequenceNo\")\n",
    "\n",
    "print(\"BroadcastLogID\" in logs.columns)  # => False\n",
    "print(\"SequenceNo\" in logs.columns)  # => False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f1ab07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cast convert to datatypes like int , stringtype ,bollean\n",
    "#substr select the string are any thing like 1 to 2 \n",
    "#distinct remove the dublicates \n",
    "\n",
    "\n",
    "logs.select(\n",
    "    F.col(\"Duration\"),\n",
    "    (\n",
    "        F.col(\"Duration\").substr(1, 2).cast(\"int\") * 60 * 60\n",
    "        + F.col(\"Duration\").substr(4, 2).cast(\"int\") * 60\n",
    "        + F.col(\"Duration\").substr(7, 2).cast(\"int\")\n",
    "tion_seconds\"),\n",
    ").distinct().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b8265d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the rdd todataframes like toDf\n",
    "logs.toDF(*[x.lower() for x in logs.columns]).printSchema()\n",
    "\n",
    "#sorting \n",
    "logs.select(sorted(logs.columns)).printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725a1b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary() and describe()  is used to statication abrivation mean,mode,std\n",
    "#min max\n",
    "\n",
    "\n",
    "#describe to show basic statics \n",
    "\n",
    "#summary provides more comprehenisve stastics\n",
    "\n",
    "for i in logs.columns:\n",
    "    logs.describe(i).show()\n",
    "    \n",
    "for i in logs.columns:\n",
    "    logs.select(i).summary().show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464a0c9c",
   "metadata": {},
   "source": [
    "# joins \n",
    "inner join , left join , right join , outter join ()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
